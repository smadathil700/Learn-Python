{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Welcome to this PySpark data cleansing tutorial! In this video, we’ll take a messy, real-world dataset and transform it into clean, ready-to-use data using just a few powerful PySpark techniques.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import SparkSession—this starts our Spark engine and manages our data.\n",
    "Next, we bring in functions as F, a toolbox full of built-in functions for cleaning and transforming columns.\n",
    "And finally, we import Window as W, which lets us create advanced analytics like ranking, deduplication, and time-based filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eb3acde-79b8-4637-b34f-a359b517eb71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import SparkSession, SQL functions, and Window specification from PySpark\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql import Window as W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a Spark session. This is like launching our data workshop—it controls the whole PySpark process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "683d4b42-6618-4cad-b8f9-3b98e5bb7bb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1)Initialize Spark Session\n",
    "\n",
    "spark = (SparkSession.builder              # Initialize a SparkSession builder\n",
    ".appName(\"DataCleaningDemo\")               # Set the Spark application name\n",
    ".config(\"spark.sql.session.timeZone\", \"UTC\")  # Configure the Spark SQL session timezone to UTC\n",
    ".getOrCreate())                            # Create a new SparkSession or get the existing one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load our messy CSV dataset, which contains duplicates, missing values, inconsistent formats, and even some extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d9b39a8-6367-45c8-b57b-444f3dc267e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) Load Messy Dataset with Anomalies\n",
    "df = (spark.read                           # Start reading data using Spark DataFrameReader\n",
    ".option(\"header\", True)                    # Treat the first row as header\n",
    ".option(\"multiLine\", True)                 # Allow multiline fields in the CSV\n",
    ".option(\"escape\", '\"')                     # Use double quotes to escape special characters\n",
    ".csv(\"/Volumes/workspace/csv_files/csv_files/dataset.csv\"))  # Load CSV file from specified path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always start by inspecting the raw data with .show() and .printSchema(). This helps us see what we’re dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ab84246-3405-47c8-8ec1-dc5a0b37a0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---------+---------------+------+---------------+\n",
      "|ID | Name |Amount($)|is Active|Order Date     |Sex   |Updated At     |\n",
      "+---+------+---------+---------+---------------+------+---------------+\n",
      "|152|Ånna  | 1 200   |Inactive |2/26/2023 22:12|FEMALE|6/1/2023 16:02 |\n",
      "|415|Renée |10.000,00|1        |1/15/2023 10:57|f     |2/13/2023 19:50|\n",
      "|119|Judy  |1,000.50 |Active   |6/2/2023 17:14 |M     |4/6/2023 15:32 |\n",
      "|365|Ivan  |-5000    |N        |2/2/2023 21:16 |f     |2/15/2023 21:07|\n",
      "|30 |Bob   |NA       |TRUE     |NULL           |Other |1/13/2023 1:20 |\n",
      "|403|Frank |500      |Active   |3/31/2023 0:58 |f     |4/6/2023 5:51  |\n",
      "|64 |David |2000     |NO       |1/16/2023 21:43|f     |6/4/2023 1:03  |\n",
      "|482|Alice |2.000,50 |Y        |6/10/2023 16:42|FEMALE|6/10/2023 20:17|\n",
      "|442|Hannah|10.000,00|Inactive |2/8/2023 14:43 |F     |2/4/2023 7:50  |\n",
      "|448|Renée |2.000,50 |Active   |5/1/2023 17:50 |f     |1/22/2023 2:21 |\n",
      "+---+------+---------+---------+---------------+------+---------------+\n",
      "only showing top 10 rows\n",
      "root\n",
      " |-- ID : string (nullable = true)\n",
      " |--  Name: string (nullable = true)\n",
      " |-- Amount($): string (nullable = true)\n",
      " |-- is Active: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Updated At: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) Inspect Data\n",
    "df.show(10, truncate=False)  # Display first 10 rows without truncating column values\n",
    "df.printSchema()             # Print the schema of the DataFrame with column types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we standardize column names—removing spaces, special characters, and making them lowercase. Clean columns make everything easier downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb64cc4-1a1b-480d-9a60-99582bad98ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- is_active: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- updated_at: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) Standardize Column Names\n",
    "\n",
    "import re                                             # Import regular expressions module\n",
    "new_cols = []                                         # Initialize list to store cleaned column names\n",
    "for c in df.columns:                                  # Iterate over each column name in the DataFrame\n",
    "    nc = re.sub(r\"[\\s\\-]+\", \"_\", c.strip().lower())   # Replace spaces/hyphens with underscores and convert to lowercase\n",
    "    nc = re.sub(r\"[^0-9a-z_]+\", \"\", nc)               # Remove any characters that are not alphanumeric or underscores\n",
    "    nc = re.sub(r\"_+\", \"_\", nc).strip(\"_\")            # Replace multiple underscores with single and strip leading/trailing underscores\n",
    "    new_cols.append(nc)                               # Append cleaned column name to the list\n",
    "for old, new in zip(df.columns, new_cols):            # Pair old and new column names for renaming\n",
    "    if old != new:                                    # Check if the column name has changed\n",
    "        df = df.withColumnRenamed(old, new)           # Rename the column in the DataFrame\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we trim spaces, remove non-breaking spaces, and replace placeholder values like ‘NA’ or dashes with proper nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7486d243-feb6-4e2f-a1cf-a006cd9becfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------+---------+---------------+------+---------------+\n",
      "|id |name  |amount   |is_active|order_date     |sex   |updated_at     |\n",
      "+---+------+---------+---------+---------------+------+---------------+\n",
      "|152|Ånna  |1 200    |Inactive |2/26/2023 22:12|FEMALE|6/1/2023 16:02 |\n",
      "|415|Renée |10.000,00|1        |1/15/2023 10:57|f     |2/13/2023 19:50|\n",
      "|119|Judy  |1,000.50 |Active   |6/2/2023 17:14 |M     |4/6/2023 15:32 |\n",
      "|365|Ivan  |-5000    |N        |2/2/2023 21:16 |f     |2/15/2023 21:07|\n",
      "|30 |Bob   |NULL     |TRUE     |NULL           |Other |1/13/2023 1:20 |\n",
      "|403|Frank |500      |Active   |3/31/2023 0:58 |f     |4/6/2023 5:51  |\n",
      "|64 |David |2000     |NO       |1/16/2023 21:43|f     |6/4/2023 1:03  |\n",
      "|482|Alice |2.000,50 |Y        |6/10/2023 16:42|FEMALE|6/10/2023 20:17|\n",
      "|442|Hannah|10.000,00|Inactive |2/8/2023 14:43 |F     |2/4/2023 7:50  |\n",
      "|448|Renée |2.000,50 |Active   |5/1/2023 17:50 |f     |1/22/2023 2:21 |\n",
      "+---+------+---------+---------+---------------+------+---------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# 5) Clean Missing Values & Strings\n",
    "\n",
    "na_tokens = [\"\", \"na\", \"n/a\", \"none\", \"null\", \"-\", \"--\", \"unknown\"]  # Define tokens representing missing values\n",
    "for c, t in df.dtypes:                                               # Iterate through each column and its data type\n",
    "    if t == 'string':                                                # Process only string columns\n",
    "        df = df.withColumn(c, F.regexp_replace(F.col(c), \"\\xa0\", \" \"))  # Replace non-breaking spaces with regular spaces\n",
    "        df = df.withColumn(c, F.trim(F.col(c)))                         # Trim leading and trailing spaces\n",
    "        df = df.withColumn(c, F.regexp_replace(F.col(c), r\"\\s+\", \" \"))  # Replace multiple spaces with a single space\n",
    "        df = df.withColumn(                                           \n",
    "            c,                                                        # For the current column\n",
    "            F.when(F.lower(F.col(c)).isin(na_tokens), None)           # Replace NA tokens with null\n",
    "             .otherwise(F.col(c))                                     # Keep the original value otherwise\n",
    "        )\n",
    "\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amounts become numbers, booleans become true or false, and dates are parsed into proper timestamps—even with mixed formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da0287ac-f6fe-4297-88e8-099b9936c75c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+---------+----------------+------+----------------+\n",
      "|id |name  |amount |is_active|order_date      |sex   |updated_at      |\n",
      "+---+------+-------+---------+----------------+------+----------------+\n",
      "|152|Ånna  |1200.0 |false    |2023-02-26 22:12|FEMALE|2023-06-01 16:02|\n",
      "|415|Renée |10000.0|true     |2023-01-15 10:57|f     |2023-02-13 19:50|\n",
      "|119|Judy  |1.0005 |true     |2023-06-02 17:14|M     |2023-04-06 15:32|\n",
      "|365|Ivan  |-5000.0|false    |2023-02-02 21:16|f     |2023-02-15 21:07|\n",
      "|30 |Bob   |NULL   |true     |NULL            |Other |2023-01-13 01:20|\n",
      "|403|Frank |500.0  |true     |2023-03-31 00:58|f     |2023-04-06 05:51|\n",
      "|64 |David |2000.0 |false    |2023-01-16 21:43|f     |2023-06-04 01:03|\n",
      "|482|Alice |2000.5 |true     |2023-06-10 16:42|FEMALE|2023-06-10 20:17|\n",
      "|442|Hannah|10000.0|false    |2023-02-08 14:43|F     |2023-02-04 07:50|\n",
      "|448|Renée |2000.5 |true     |2023-05-01 17:50|f     |2023-01-22 02:21|\n",
      "+---+------+-------+---------+----------------+------+----------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# 6) Convert Data Types\n",
    "\n",
    "# Amount to Double (handle full-width digits)\n",
    "\n",
    "df = df.withColumn(\"amount\", F.regexp_replace(\"amount\", \"０\", \"0\"))             # Replace full-width zero with normal zero\n",
    "df = df.withColumn(\"amount\", F.regexp_replace(\"amount\", r\"[^0-9,\\.-]\", \"\"))     # Remove all non-numeric, non-comma, non-dot, non-dash characters\n",
    "df = df.withColumn(\"amount\", F.regexp_replace(\"amount\", r\"\\.\", \"\"))             # Remove periods used as thousands separators\n",
    "df = df.withColumn(\"amount\", F.regexp_replace(\"amount\", r\",\", \".\"))             # Replace comma with dot for decimal conversion\n",
    "df = df.withColumn(\"amount\", F.col(\"amount\").cast(\"double\"))                    # Cast cleaned amount column to double type\n",
    "\n",
    "# Boolean\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"is_active\",\n",
    "    F.when(F.lower(\"is_active\").isin([\"true\",\"t\",\"yes\",\"y\",\"1\",\"active\"]), F.lit(True))    # Map common truthy values to True\n",
    "     .when(F.lower(\"is_active\").isin([\"false\",\"f\",\"no\",\"n\",\"0\",\"inactive\"]), F.lit(False)) # Map common falsy values to False\n",
    "     .otherwise(F.lit(None).cast(\"boolean\"))                                               # Set all other/unrecognized values to NULL\n",
    ")\n",
    "\n",
    "# Dates\n",
    "\n",
    "# order_date\n",
    "\n",
    "from pyspark.sql.functions import col, trim, regexp_replace, when, coalesce, lit, expr\n",
    "\n",
    "# Step 1: Trim spaces\n",
    "df = df.withColumn(\"order_date\", trim(col(\"order_date\")))                              # Remove leading/trailing spaces from order_date\n",
    "\n",
    "# Step 2: Normalize separators (- or . → /)\n",
    "df = df.withColumn(\"order_date\", regexp_replace(col(\"order_date\"), \"[-.]\", \"/\"))      # Standardize date separators to \"/\"\n",
    "\n",
    "# Step 3: Fill missing time only if date exists\n",
    "df = df.withColumn(\n",
    "    \"order_date\",\n",
    "    when(col(\"order_date\").isNotNull() & (~col(\"order_date\").rlike(r\"\\d{1,2}/\\d{1,2}/\\d{4}\\s\\d{1,2}:\\d{2}\")),\n",
    "         concat_ws(\" \", col(\"order_date\"), lit(\"00:00\")))                               # Append \"00:00\" if time is missing\n",
    "    .otherwise(col(\"order_date\"))                                                      # Keep original value if time exists or NULL\n",
    ")\n",
    "\n",
    "# Step 4: Safely parse timestamp (invalid formats become NULL)\n",
    "df = df.withColumn(\n",
    "    \"order_date_ts\",\n",
    "    expr(\"try_to_timestamp(order_date, 'M/d/yyyy H:mm')\")                               # Convert to timestamp, invalid strings become NULL\n",
    ")\n",
    "\n",
    "# Step 5: Format timestamp as string YYYY-MM-DD HH:MM (no seconds)\n",
    "df = df.withColumn(\n",
    "    \"order_date\",\n",
    "    when(col(\"order_date_ts\").isNotNull(), F.date_format(col(\"order_date_ts\"), \"yyyy-MM-dd HH:mm\"))  # Format timestamp to string without seconds\n",
    "    .otherwise(None)                                                                   # Keep NULL if parsing failed\n",
    ").drop(\"order_date_ts\")                                                                 # Drop intermediate timestamp column\n",
    "\n",
    "# updated_at\n",
    "\n",
    "# Step 1: Trim spaces\n",
    "df = df.withColumn(\"updated_at\", trim(col(\"updated_at\")))                              # Remove leading/trailing spaces from updated_at\n",
    "\n",
    "# Step 2: Normalize separators (- or . → /)\n",
    "df = df.withColumn(\"updated_at\", regexp_replace(col(\"updated_at\"), \"[-.]\", \"/\"))      # Standardize date separators to \"/\"\n",
    "\n",
    "# Step 3: Fill missing time only if date exists\n",
    "df = df.withColumn(\n",
    "    \"updated_at\",\n",
    "    when(col(\"updated_at\").isNotNull() & (~col(\"updated_at\").rlike(r\"\\d{1,2}/\\d{1,2}/\\d{4}\\s\\d{1,2}:\\d{2}\")),\n",
    "         concat_ws(\" \", col(\"updated_at\"), lit(\"00:00\")))                               # Append \"00:00\" if time is missing\n",
    "    .otherwise(col(\"updated_at\"))                                                      # Keep original value if time exists or NULL\n",
    ")\n",
    "\n",
    "# Step 4: Safely parse timestamp (invalid formats become NULL)\n",
    "df = df.withColumn(\n",
    "    \"updated_at_ts\",\n",
    "    expr(\"try_to_timestamp(updated_at, 'M/d/yyyy H:mm')\")                              # Convert to timestamp, invalid strings become NULL\n",
    ")\n",
    "\n",
    "# Step 5: Format timestamp as string YYYY-MM-DD HH:MM (no seconds)\n",
    "df = df.withColumn(\n",
    "    \"updated_at\",\n",
    "    when(col(\"updated_at_ts\").isNotNull(), F.date_format(col(\"updated_at_ts\"), \"yyyy-MM-dd HH:mm\"))  # Format timestamp to string without seconds\n",
    "    .otherwise(None)                                                                   # Keep NULL if parsing failed\n",
    ").drop(\"updated_at_ts\")                                                                 # Drop intermediate timestamp column\n",
    "\n",
    "df.show(10, truncate=False)                                                               # Display first 10 rows without truncating columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove duplicates by keeping the latest record based on the ‘updated_at’ timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab981f68-6d7a-4487-8212-d2db58a361e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7) Deduplicate\n",
    "\n",
    "pk = [\"id\"]                                                                                 # Define primary key column(s) for identifying duplicates\n",
    "if \"updated_at\" in df.columns:                                                              # Check if updated_at column exists\n",
    "    w = W.partitionBy(*pk).orderBy(F.col(\"updated_at\").desc())                               # Define window: partition by PK and order by updated_at descending\n",
    "    df = (df.withColumn(\"_rn\", F.row_number().over(w))                                        # Add row number within each PK partition\n",
    "            .where(F.col(\"_rn\") == 1)                                                       # Keep only the latest row per PK based on updated_at\n",
    "            .drop(\"_rn\"))                                                                   # Drop the temporary row number column\n",
    "else:\n",
    "    df = df.dropDuplicates(pk)                                                              # If updated_at doesn't exist, drop duplicate rows based on PK only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sex values like ‘M’, ‘male’, or even ‘FEMALE’ are unified into consistent categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4655effe-3d89-45da-92ee-fb62e8d18794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+---------+----------------+------+----------------+\n",
      "|id |name |amount |is_active|order_date      |sex   |updated_at      |\n",
      "+---+-----+-------+---------+----------------+------+----------------+\n",
      "|1  |Frank|NULL   |false    |2023-06-07 03:11|Male  |2023-06-11 02:35|\n",
      "|10 |Renée|1200.0 |true     |2023-01-15 08:53|NULL  |2023-05-03 23:50|\n",
      "|100|Eve  |2000.0 |false    |2023-03-15 01:46|Other |2023-03-13 01:07|\n",
      "|101|Alice|5.5    |false    |2023-03-20 23:07|Male  |2023-05-31 14:31|\n",
      "|103|Renée|NULL   |true     |2023-02-12 02:34|NULL  |2023-05-31 04:59|\n",
      "|105|Ivan |-5000.0|false    |NULL            |NULL  |2023-04-18 22:24|\n",
      "|106|Judy |NULL   |true     |2023-01-13 23:58|NULL  |2023-01-14 01:26|\n",
      "|107|Alice|300.0  |false    |2023-06-07 01:30|Female|2023-04-13 04:44|\n",
      "|108|Ånna |1.0005 |false    |2023-01-08 09:53|Female|2023-05-12 12:52|\n",
      "|109|Bob  |2000.0 |false    |2023-05-16 11:17|NULL  |2023-05-22 07:59|\n",
      "+---+-----+-------+---------+----------------+------+----------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# 8) Normalize Categories\n",
    "\n",
    "m = {\"m\":\"Male\",\"male\":\"Male\",\"f\":\"Female\",\"female\":\"Female\",\"o\":\"Other\",\"other\":\"Other\"}  # Define mapping of various category variants to standardized values\n",
    "map_expr = F.create_map([F.lit(x) for kv in m.items() for x in kv])                        # Create a Spark map literal from the dictionary for lookups\n",
    "df = df.withColumn(\"sex\", F.coalesce(F.element_at(map_expr, F.lower(\"sex\")), F.col(\"sex\"))) # Normalize 'sex' column using the map; fallback to original value if not in map\n",
    "\n",
    "\n",
    "df.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extreme values are detected using the interquartile range and capped to keep them in a reasonable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58fbc204-783e-46ce-bb01-973a6502a6b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+---------+----------------+------+----------------+-------------+\n",
      "|id |name |amount |is_active|order_date      |sex   |updated_at      |amount_capped|\n",
      "+---+-----+-------+---------+----------------+------+----------------+-------------+\n",
      "|1  |Frank|NULL   |false    |2023-06-07 03:11|Male  |2023-06-11 02:35|NULL         |\n",
      "|10 |Renée|1200.0 |true     |2023-01-15 08:53|NULL  |2023-05-03 23:50|1200.0       |\n",
      "|100|Eve  |2000.0 |false    |2023-03-15 01:46|Other |2023-03-13 01:07|2000.0       |\n",
      "|101|Alice|5.5    |false    |2023-03-20 23:07|Male  |2023-05-31 14:31|5.5          |\n",
      "|103|Renée|NULL   |true     |2023-02-12 02:34|NULL  |2023-05-31 04:59|NULL         |\n",
      "|105|Ivan |-5000.0|false    |NULL            |NULL  |2023-04-18 22:24|-2987.0      |\n",
      "|106|Judy |NULL   |true     |2023-01-13 23:58|NULL  |2023-01-14 01:26|NULL         |\n",
      "|107|Alice|300.0  |false    |2023-06-07 01:30|Female|2023-04-13 04:44|300.0        |\n",
      "|108|Ånna |1.0005 |false    |2023-01-08 09:53|Female|2023-05-12 12:52|1.0005       |\n",
      "|109|Bob  |2000.0 |false    |2023-05-16 11:17|NULL  |2023-05-22 07:59|2000.0       |\n",
      "+---+-----+-------+---------+----------------+------+----------------+-------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# 9) Handle Outliers\n",
    "\n",
    "q1, q3 = df.approxQuantile(\"amount\", [0.25, 0.75], 0.01)                # Compute approximate 25th and 75th percentiles (Q1 and Q3) of 'amount$'\n",
    "iqr = q3 - q1                                                              # Calculate interquartile range (IQR)\n",
    "lo, hi = q1 - 1.5*iqr, q3 + 1.5*iqr                                        # Define lower and upper bounds for outliers (1.5*IQR rule)\n",
    "df = df.withColumn(\n",
    "    \"amount_capped\",\n",
    "    F.when(F.col(\"amount\") < lo, lo)                                      # Cap values below lower bound to the lower bound\n",
    "     .when(F.col(\"amount\") > hi, hi)                                      # Cap values above upper bound to the upper bound\n",
    "     .otherwise(F.col(\"amount\"))                                           # Keep values within bounds unchanged\n",
    ")\n",
    "\n",
    "\n",
    "df.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add helpful columns like the order month for easier time-based reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b545dd90-d692-4c42-b45a-e2111089991a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10) Add Date Ranges\n",
    "df = df.withColumn(\"order_month\", F.date_trunc(\"MONTH\", \"order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we validate that there are no missing primary keys and save the cleaned dataset as an efficient Parquet file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d8dca5c-1634-404e-9cad-7cd5df9db1db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 11) Validate\n",
    "\n",
    "assert df.filter(F.col(\"id\").isNull()).count() == 0        # Ensure there are no rows with NULL in the 'id' column\n",
    "assert df.count() > 0                                     # Ensure the DataFrame is not empty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that’s it! You’ve just cleaned a complex dataset with PySpark—step by step. Don’t forget to like, share, and subscribe for more PySpark tutorials!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "344b6cc8-962c-49f2-adaf-9d620b39fd10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"default.processed_data\")\n",
    "\n",
    "# Save the DataFrame as a Delta table named 'default.processed_data', overwriting existing data and merging the schema if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e404b80-d140-4a1d-a35b-19dbdebf1620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_prep",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
